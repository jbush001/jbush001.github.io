---
layout: post
title: Write-back vs. write through bake-off
date: '2014-07-05T16:57:00.002-07:00'
author: Jeff
tags:
- microarchitecture
- cache coherence
- gpgpu
- profiling
modified_time: '2016-01-22T18:55:40.835-08:00'
thumbnail: http://1.bp.blogspot.com/-oIeMRC7N0Tk/U83Ee3Eta1I/AAAAAAAABjU/cM40uYi2hLU/s72-c/Screen+Shot+2014-07-21+at+6.54.21+PM.png
blogger_id: tag:blogger.com,1999:blog-5853447763770338628.post-3089316913994586063
blogger_orig_url: http://latchup.blogspot.com/2014/07/write-back-vs-write-through-bake-off.html
---

<div class="separator" style="clear: both; text-align: left;">In the last update, I was debating whether the complexity of a write-back cache was justified by the performance increase. I ran a little bake-off between the version 1 microarchitecture, which uses a write through L1 cache, and the in-progress version 2 using a write-back L1 cache. There are many other differences between the architectures which would affect performance, but I'm going to focus on memory access patterns, which should be similar. &nbsp;I used the same teapot benchmark as in previous posts, as it has a nice mix of memory accesses. Both versions are running one core, so there are aspects of cache coherence traffic that aren't captured, but I'll get to that in a minute.</div><div class="separator" style="clear: both; text-align: left;"></div><a name='more'></a><br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="{{ site.url }}/assets/2014-07-05-image-0000.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="{{ site.url }}/assets/2014-07-05-image-0000.png" /></a></div><br /><div>In this chart, the top bar represents the version 1 write through architecture. The number of read misses are roughly comparable. The green represents memory write instructions. As mentioned previously, all memory writes go through the L2 cache and thus consume interconnect bandwidth.</div><div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">The bottom bar represents the version 2 write-back architecture. In this case, the green bar has been replaced by the red and orange parts. A write invalidate occurs when the cache line is not present in the l1 cache, or when it is the cache in the shared state and needs to be promoted to the exclusive state in order to be writable. &nbsp;The orange bar represents writebacks of dirty lines that were flushed or evicted from the L1 cache.</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">Overall, for this use case, the write-back architecture reduces interconnect traffic by 62%. &nbsp;This presumably allows double the number of cores to share the same L2 cache, assuming the system is interconnect traffic bound.</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">However, this is somewhat an ideal case because it doesn't cover cases where cache lines are shared. In the write-back architecture, whenever a core writes to a cache line, it must send a message to remove it from other caches. If those subsequently attempt to read the cache line again, they will cause a cache miss. This can occur either for addresses that are legitimately used for communication between threads or collateral damage due to "false sharing," where unrelated variables happen to be on the same cache line as a shared variable. &nbsp;The performance of the write-through architecture does not change as sharing increases, because it essentially always assumes lines are shared.&nbsp;</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">This analysis is also potentially overly pessimistic about write through protocols. &nbsp;Write combining (which was not supported by the V1 architecture) can reduce the number of interconnect messages, more so if multiple store buffers are employed. However, this heavily dependent on usage patterns and it difficult to perform back-of-napkin estimates for.</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">In both tests the amount of interconnect traffic generated by each core is relatively low, a single core utilizes 7% of interconnect cycles for v1 and 2.4% for v2. Of course, this use case may not be fully representative. It would be useful to see how other workloads compare.</div><div class="separator" style="clear: both;"><br /></div></div>