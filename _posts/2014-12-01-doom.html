---
layout: post
title: Doom
date: '2014-12-01T22:37:00.000-08:00'
author: Jeff
tags:
- simulation
- verilog
- doom
modified_time: '2016-01-06T21:36:51.914-08:00'
thumbnail: http://4.bp.blogspot.com/-2AGiN1ztWbg/VH1VC597ijI/AAAAAAAABuY/ZyzIIcP8mi4/s72-c/Screen%2BShot%2B2014-12-01%2Bat%2B9.58.14%2BPM.png
blogger_id: tag:blogger.com,1999:blog-5853447763770338628.post-5232711682420685657
blogger_orig_url: http://latchup.blogspot.com/2014/12/doom.html
---

<div class="separator" style="clear: both; text-align: left;">How many instructions does a processor need to execute to get through the first level of Doom? &nbsp;About 2.7 billion, in my case. &nbsp;Over the course of the several minutes it took me to stumble through the level, it rendered about 1080 frames, each of which required about 2.5 million instructions to produce. The other 700 million instructions initialized the game, loading textures, setting up data structures, etc.&nbsp;</div><div class="separator" style="clear: both; text-align: left;"></div><a name='more'></a><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">I ported Doom to my <a href="https://github.com/jbush001/NyuziProcessor" target="_blank">GPGPU processor</a>&nbsp;recently. While it doesn't use advanced hardware features of this architecture like hardware multithreading, SIMD, or even floating point, it is a good test of the toolchain and libraries, being substantially larger than other test programs I've run. &nbsp;It did shake out some good bugs.</div><div class="separator" style="clear: both; text-align: center;"><br /></div><div style="text-align: center;"><iframe allowfullscreen="" frameborder="0" height="315" src="//www.youtube.com/embed/djR_44OvYcU?rel=0&amp;showinfo=0" width="420"></iframe></div><div style="text-align: center;"><i>I forgot how much I sucked at this game</i></div><br /><div class="separator" style="clear: both; text-align: left;">The nice thing about the instruction set simulator (emulator) is that I can instrument at a fairly fine grained level. &nbsp;Here is breakdown of instructions by type:</div><div class="separator" style="clear: both; text-align: left;"><a href="{{ site.url }}/assets/2014-12-01-image-0000.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em; text-align: center;"><img border="0" height="352" src="{{ site.url }}/assets/2014-12-01-image-0000.png" width="640" /></a></div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">Interestingly, this matches the instruction profile of the 3D teapot renderer I wrote within a few percent:</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="p1">load 22.71%</div><div class="p1">store 10.67%</div><div class="p1">branch 11.81%</div><div class="p1">arithmetic 54.45%</div><div class="separator" style="clear: both; text-align: left;"></div><div class="p1"><br /></div><div class="p1">I would have expected a program designed for a different generation of hardware to have a different instruction profile (for example, Doom uses lookup tables heavily, which I'd presume would make it more weighted towards memory loads). However, that doesn't seem to be the case. Perhaps the instruction distribution is influenced more by the compiler.</div><div class="p1"><br /></div><div class="separator" style="clear: both; text-align: left;">The simulator gets around 8 frames per second on my Core i5 laptop. &nbsp;That means it's executing around 20 million instructions per second on average. &nbsp;The simulator isn't optimized, as I've mostly targeted it as a reference for co-verification of the hardware model, so that's not too bad, I guess.</div><br />However, it's a speed demon compared to the cycle accurate Verilog simulation, which shlumps along at the equivalent of 73kHz on my laptop. This is no fault of <a href="http://www.veripool.org/wiki/verilator" target="_blank">Verilator</a>, the open source tool I'm using to compile the model, which is actually very fast relative to other simulators. It's just simulating the model at a high level of detail. I managed to get it to initialize and render the first frame of the level over the course of an hour and 20 minutes. &nbsp;During that time, it executed around 353 million clock cycles.<br /><br /><table><tbody><tr><td>total cycles</td><td>353,708,911 </td></tr><tr><td>l2_writeback</td><td>88,342 </td></tr><tr><td>l2_miss</td><td>121,008 </td></tr><tr><td>l2_hit</td><td>4,505,554 </td></tr><tr><td>store rollback count</td><td>2,200,227 </td></tr><tr><td>store count</td><td>6,519,751 </td></tr><tr><td>instruction_retire</td><td>77,984,252 </td></tr><tr><td>instruction_issue</td><td>93,883,016 </td></tr><tr><td>l1i_miss</td><td>2,133 </td></tr><tr><td>l1i_hit</td><td>158,202,234 </td></tr><tr><td>l1d_miss</td><td>304,961 </td></tr><tr><td>l1d_hit</td><td>9,045,137 </td></tr></tbody></table><div class="separator" style="clear: both; text-align: left;"><br /></div>The performance is pretty crummy. It only issues instructions about 26% of clock cycles, the rest being idle, presumably because it is waiting on memory. &nbsp;Of the issued instructions, only 83% are retired. It ends up rolling back the rest because they were speculatively issued and couldn't complete.<br /><br />I'd estimate it would get a little over 5 frames per second running on FPGA at 50Mhz. &nbsp;This architecture depends on hardware threading to hide latency and makes little effort otherwise to minimize it. It's designed for highly parallel workloads. &nbsp;If the game utilized a real 3D renderer, those threads could be put to work rendering the scene in parallel.<br /><br /><br />